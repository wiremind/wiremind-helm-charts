nameOverride: ""
fullnameOverride: ""

## To define common settings, it may be useful to use YAML anchors to use the same
## base settings for all node types (data + master) + ingest
es-defaults: &defaults
  ## Change me, and change urls using elasticsearch below
  clusterName: "my-elasticsearch-cluster"
  ## Enable if you want backup (see below)
  # keystore:
  #   - secretName: $RELEASE-NAME-elasticsearch-s3
  ## Installs by default repository-s3 and mappersize plugin
  elasticsearchPlugins: repository-s3 mapper-size
  extraInitContainers: |
    {{- if .Values.elasticsearchPlugins }}
    - name: elasticsearch-install-plugins
      image: {{ .Values.image }}:{{ .Values.imageTag }}
      command:
        - sh
      args:
        - -c
        - |
          set -e; set -x;
          for PLUGIN_NAME in "{{ .Values.elasticsearchPlugins }}"; do
            echo "Installing $PLUGIN_NAME..."
            PLUGIN_LIST=$(bin/elasticsearch-plugin list)
            echo $PLUGIN_LIST
            if echo $PLUGIN_LIST | grep "$PLUGIN_NAME"; then
              echo "Plugin $PLUGIN_NAME already exists, skipping."
            else
              echo "$PLUGIN_NAME does not exist yet, installing..."
              bin/elasticsearch-plugin install --batch $PLUGIN_NAME
            fi
          done
      volumeMounts:
        - mountPath: /usr/share/elasticsearch/plugins/
          name: plugindir
    {{- end }}
  extraVolumes: |
    - name: plugindir
      emptyDir: {}
  extraVolumeMounts: |
    - mountPath: /usr/share/elasticsearch/plugins/
      name: plugindir
  securityContext:
    allowPrivilegeEscalation: false
    privileged: false
  createCert: true
  transportTls:
    verificationMode: none
  httpTls:
    enabled: false
  protocol: http
  tests:
    enabled: false
  secret:
    # Change me or else it will be auto-generated
    password: ""

## The data nodes
es-data-hot:
  <<: *defaults
  nodeGroup: "data-hot"
  enabled: true
  # esConfig:
  #   elasticsearch.yml: |
  #     node:
  #       attr:
  #         data: hot
  roles:
    - ingest
    - data_hot
    - data_content
  replicas: 1
  # esJavaOpts: "-Xmx1g  -Xms1g"
  # resources:
  #   requests:
  #     cpu: "10m"
  #     memory: "128Mi"
  #   limits:
  #     memory: "56Gi"
  #     cpu: 15
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi
  ingress:
    enabled: false
    # path: /
    # hosts:
    #   - elasticsearch.setme.org
    # tls:
    #   - secretName: elasticsearch.setme.org-tls
    #     hosts:
    #       - elasticsearch.setme.org

es-data-warm:
  <<: *defaults
  nodeGroup: "data-warm"
  enabled: false
  # esConfig:
  #   elasticsearch.yml: |
  #     node:
  #       attr:
  #         data: warm
  roles:
    - data_warm
  replicas: 1
  esJavaOpts: "-Xmx1g  -Xms1g"
  # resources:
  #   requests:
  #     memory: "128Mi"
  #     cpu: "10m"
  #   limits:
  #     memory: "18Gi"
  #     cpu: 15
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

es-data-cold:
  <<: *defaults
  nodeGroup: "data-cold"
  enabled: false
  # esConfig:
  #   elasticsearch.yml: |
  #     node:
  #       attr:
  #         data: cold
  roles:
    - data_cold
  replicas: 1
  # esJavaOpts: "-Xmx1g  -Xms1g"
  # resources:
  #   requests:
  #     memory: "128Mi"
  #     cpu: "10m"
  #   limits:
  #     memory: 16Gi
  #     cpu: 6
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

## The ingest (a.k.a client) nodes
es-ingest:
  <<: *defaults
  nodeGroup: "ingest"
  enabled: false
  roles:
    - ingest
  replicas: 1
  # esJavaOpts: "-Xmx1g  -Xms1g"
  # resources:
  #   requests:
  #     memory: 128Mi
  #     cpu: "10m"
  #   limits:
  #     memory: "1Gi"
  #     cpu: "2"
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

## The master nodes
es-master:
  <<: *defaults
  nodeGroup: "master"
  enabled: true
  roles:
    - master
  # esJavaOpts: "-Xms128m -Xmx128m"
  #  resources:
  #    requests:
  #      memory: 128Mi
  #      cpu: "10m"
  #    limits:
  #      memory: "1Gi"
  #      cpu: "2"
  replicas: 1
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

## Creates a default indexLifecycleManagement policy
## Note: you'll have to manage index templates so that your indices point to this ILM.
indexLifecycleManagement:
  enabled: false
  ## Name of the policy to create
  name: wiremind
  ## Ref: https://www.elastic.co/guide/en/elasticsearch/reference/current/_timing.html
  warm:
    minAge: 8d
  cold:
    minAge: 31d
  delete:
    enabled: false
    minAge: 90d
  ## priority class to use for the job setting up the ILM
  # priorityClassName: my-priority-class

## Configure a backup repository and snapshot policy on it
keystore:
  ## Create a secret containing credentials
  ## Currently only supports s3
  ## You can manually create a secret instead
  enabled: false
  accessKey: setme  # change me
  secretKey: setme  # change me

backup:
  enabled: false
  ## Ref:
  ## https://www.elastic.co/guide/en/elasticsearch/reference/7.x/snapshots-register-repository.html
  repository:
    ## Will setup the repository if enabled and backup is enabled
    ## Note that elasticseach repository name is stated in snapshotPolicy below
    enabled: true
    ## Note: currently, only s3 is supported
    type: s3
    settings:
      bucket: setme  # change me
      region: eu-west-3
      endpoint: s3.eu-west-3.amazonaws.com
  snapshotPolicyName: snapshot
  ## Ref:
  ## https://www.elastic.co/guide/en/elasticsearch/reference/7.x/getting-started-snapshot-lifecycle-management.html
  ## https://www.elastic.co/guide/en/elasticsearch/reference/7.x/slm-api-put-policy.html
  snapshotPolicy:
    ## Will setup the snapshot policy if enabled and backup is enabled
    enabled: true
    schedule: "0 30 1 * * ?"
    name: <snapshot-{now/d}>
    repository: backup  ## Also sets repository name. Beware when changing name, you'll have duplicate registered repositories and you manually need to delete the old one
    config:
      indices:
        - "*"
      ignore_unavailable: false
      include_global_state: true
    retention:
      expire_after: 30d
      min_count: 5
      max_count: 50
  ## priority class to use for the job setting up the repository and snapshot policy
  # priorityClassName:

kibana:
  enabled: false
  imageTag: 8.15.0
  ## Change me
  elasticsearchHosts: http://my-elasticsearch-cluster-data-hot:9200
  elasticsearchCredentialSecret: my-elasticsearch-cluster-data-hot-credentials
  elasticsearchCertificateSecret: ""
  # resources:
  #   limits:
  #     cpu: 2
  #     memory: 2Gi
  #   requests:
  #     cpu: 100m
  #     memory: 1000Mi
  kibanaConfig:
    kibana.yml: |
      server.name: kibana
      server.host: "0"
      elasticsearch.requestTimeout: 180000
      xpack.security.authc.providers:
        anonymous.anonymous1:
          order: 0
          credentials:
            username: "kibana_user"
            password: "kibana_user"
        basic.basic1:
          order: 1
  ingress:
    enabled: false
    # hosts:
    #   - host: kibana.setme.org
    #     paths:
    #       - path: /
    # tls:
    #   - hosts:
    #       - kibana.setme.org
    #     secretName: kibana.setme.org-tls
  readinessProbe:
    successThreshold: 1

prometheus-elasticsearch-exporter:
  enabled: true
  es:
    ## Change me
    uri: http://my-elasticsearch-master-url:9200
  # resources:
  #   requests:
  #     cpu: 10m
  #     memory: 128Mi
  #   limits:
  #     cpu: 2
  #     memory: 128Mi
  serviceMonitor:
    enabled: false
  prometheusRule:
    enabled: false
    rules:
      - alert: ESNoRecentSnapshot
        ## Note that it will fire only if snapshot repository exists
        expr: |
          (
            time()
            -
            elasticsearch_snapshot_stats_snapshot_start_time_timestamp{service="{{ template "elasticsearch-exporter.fullname" . }}", state="SUCCESS"}
          ) / 3600 / 24 > 8
        labels:
          severity: warning
        annotations:
          summary: |
            No successful {{ "{{ $labels.instance }}" }} Elasticsearch snapshot for {{ "{{ $value }}" }} days
          description: No recent Elasticsearch backup
      - alert: ESClusterStatusRED
        expr: |
          elasticsearch_cluster_health_status{color="red", service="{{ template "elasticsearch-exporter.fullname" . }}"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Cluster health status is RED"
          description: 'Cluster {{ "{{ $labels.cluster }}" }} health status is RED'
      - alert: ESClusteStatusYELLOW
        expr: |
          elasticsearch_cluster_health_status{color="yellow", service="{{ template "elasticsearch-exporter.fullname" . }}"} == 1
        for: 20m
        labels:
          severity: high
        annotations:
          summary: "Cluster health status is YELLOW"
          description: 'Cluster {{ "{{ $labels.cluster }}" }} health status is YELLOW'

      ## Report any rejected request.
      - alert: ESBulkRequestsRejection
        expr: |
          irate(elasticsearch_thread_pool_rejected_count{service="{{ template "elasticsearch-exporter.fullname" . }}"}[1m]) > 0
        for: 1s
        labels:
          severity: high
        annotations:
          summary: "Elasticsearch Bulk Query Rejection"
          description: |
            'Bulk Rejection at {{ "{{ $labels.name }}" }} node in {{ "{{ $labels.cluster }}" }} cluster'

      - alert: ESJVMHeapHigh
        expr: |
          sum by (cluster, name, namespace)
            (elasticsearch_jvm_memory_used_bytes{area="heap", service="{{ template "elasticsearch-exporter.fullname" . }}"}
            /
            elasticsearch_jvm_memory_max_bytes{area="heap", service="{{ template "elasticsearch-exporter.fullname" . }}"}
          )
          > 75
        for: 5m
        labels:
          severity: alert
        annotations:
          summary: "JVM Heap usage on the node is high"
          description: |
            'JVM Heap usage on the node {{ "{{ $labels.node }}" }} in {{ "{{ $labels.cluster }}" }} cluster is {{ "{{ $value }}" }}%. There might be long running GCs now.'

cerebro:
  enabled: false
  ingress:
    enabled: false
  deployment:
    readinessProbe:
      enabled: false
  config:
    basePath: "/"
    restHistorySize: 50
    hosts:
      - name: elasticsearch-cluster
        ## Change me
        host: http://my-elasticsearch-cluster-data-hot:9200

## Array of extra K8s manifests to deploy
extraObjects: []

## List of queries that will be executed when kibana is up and running
setup:
  enabled: false
  terminationGracePeriodSeconds: 0
  ttlSecondsAfterFinished: 1800
  # priorityClassName: my-priority-class
  annotations:
    "helm.sh/hook": post-install, post-upgrade
    # "helm.sh/hook-weight": "1"
    # "helm.sh/hook-delete-policy": "before-hook-creation, hook-succeeded"
  queries:
    ## An example:
    # 1ClusterSettings:
    #   # Increase watermark compared to defaults
    #   enabled: true
    #   debug: false
    #   endpoint: "_cluster/settings"
    #   method: "PUT"
    #   headers:
    #     - "Content-Type: application/json"
    #   query:
    #     persistent:
    #       cluster:
    #         routing:
    #           allocation:
    #             disk:
    #               watermark:
    #                 low: 90%
    #                 high: 95%
    5CreateKibanaUser:
      enabled: true
      endpoint: "_security/user/kibana_user"
      method: POST
      headers:
        - "Content-Type: application/json"
      query:
        password: "kibana_user"
        roles:
          - kibana_user
