apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ template "pghoard.fullname" . }}-restore
  labels:
    app.kubernetes.io/name: {{ include "pghoard.name" . }}
    helm.sh/chart: {{ include "pghoard.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/component: restore
spec:
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "pghoard.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
      app.kubernetes.io/component: restore
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "pghoard.name" . }}
        helm.sh/chart: {{ include "pghoard.chart" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/managed-by: {{ .Release.Service }}
        app.kubernetes.io/component: restore
        {{ .Release.Name }}-postgresql-client: "true"  # Allows postgresql netpol, from the postgresql helm chart, to accept pghoard
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
    spec:
{{- if .Values.priorityClassName }}
      priorityClassName: "{{ .Values.priorityClassName }}"
{{- end }}
      containers:
        - name: {{ .Chart.Name }}-restore
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          command: ['/bin/bash', '/restore.sh']
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
            - name: PGHOARD_RESTORE_SITE
              value: {{ template "pghoard.fullname" . }}
          envFrom:
          - secretRef:
              name: {{ template "pghoard.fullname" . }}
          - configMapRef:
              name: {{ template "pghoard.fullname" . }}
          resources:
{{ toYaml .Values.restore.resources | indent 12 }}
          volumeMounts:
            - name: pghoard
              mountPath: /var/lib/pghoard
      restartPolicy: Always
      volumes:
        - name: pghoard
          # Note: see https://github.com/kubernetes/kubernetes/issues/60903
          persistentVolumeClaim:
            claimName: data-{{ template "pghoard.fullname" . }}-0
            # pghoard in restore mode will write a satusfile in the site directory.
            # XXX for integrity reasons, this should be set to true. We do not want a restore to mess with our backup.
            readOnly: false
    {{- with .Values.nodeSelector }}
      nodeSelector:
{{ toYaml . | indent 8 }}
    {{- end }}
      affinity:
        # Force to be on same node than backup in order to be able to mount Persistent Volume
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: {{ include "pghoard.name" . }}
                app.kubernetes.io/instance: {{ .Release.Name }}
                app.kubernetes.io/component: backup
            topologyKey: kubernetes.io/hostname
    {{- with .Values.affinity }}
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
{{ toYaml . | indent 8 }}
    {{- end }}
