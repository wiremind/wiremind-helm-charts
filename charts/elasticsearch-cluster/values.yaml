# To define common settings, it may be useful to use YAML anchors to use the same
# base settings for all node types (data + master) + ingest
es-defaults: &defaults
  clusterName: "my-elasticsearch-cluster"  # Change me

# The data nodes
es-data-hot:
  <<: *defaults
  nodeGroup: "data-hot"
  enabled: true
  # esConfig:
  #   elasticsearch.yml: |
  #     node:
  #       attr:
  #         data: hot
  #     xpack:
  #       security:
  #         enabled: false
  #         transport:
  #           ssl:
  #             enabled: false
  #         http:
  #           ssl:
  #             enabled: false
  roles:
    master: "false"
    ingest: "true"
    data: "true"
  replicas: 1
  esJavaOpts: "-Xmx128m  -Xms128m"
  # resources:
  #   requests:
  #     cpu: "10m"
  #     memory: "128Mi"
  #   limits:
  #     memory: "56Gi"
  #     cpu: 15
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi
  ingress:
    enabled: false
    # path: /
    # hosts:
    #   - elasticsearch.setme.org
    # tls:
    #   - secretName: elasticsearch.setme.org-tls
    #     hosts:
    #       - elasticsearch.setme.org

es-data-warm:
  <<: *defaults
  nodeGroup: "data-warm"
  enabled: false
  # esConfig:
  #   elasticsearch.yml: |
  #     node:
  #       attr:
  #         data: warm
  #     xpack:
  #       security:
  #         enabled: false
  #         transport:
  #           ssl:
  #             enabled: false
  #         http:
  #           ssl:
  #             enabled: false
  roles:
    master: "false"
    ingest: "false"
    data: "true"
  replicas: 1
  esJavaOpts: "-Xmx128m -Xms128m"
  # resources:
  #   requests:
  #     memory: "128Mi"
  #     cpu: "10m"
  #   limits:
  #     memory: "18Gi"
  #     cpu: 15
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

es-data-cold:
  <<: *defaults
  nodeGroup: "data-cold"
  enabled: false
  # esConfig:
  #   elasticsearch.yml: |
  #     node:
  #       attr:
  #         data: cold
  #     xpack:
  #       security:
  #         enabled: false
  #         transport:
  #           ssl:
  #             enabled: false
  #         http:
  #           ssl:
  #             enabled: false
  roles:
    master: "false"
    ingest: "false"
    data: "true"
  replicas: 1
  esJavaOpts: "-Xmx128m -Xms128m"
  # resources:
  #   requests:
  #     memory: "128Mi"
  #     cpu: "10m"
  #   limits:
  #     memory: 16Gi
  #     cpu: 6
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

# The ingest (a.k.a client) nodes
es-ingest:
  <<: *defaults
  nodeGroup: "master"
  enabled: false
  roles:
    master: "false"
    ingest: "true"
    data: "false"
  replicas: 1
  esJavaOpts: "-Xms128m -Xmx128m"
  resources:
    requests:
      memory: 128Mi
      cpu: "10m"
    limits:
      memory: "1Gi"
      cpu: "2"
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

# The master nodes
es-master:
  <<: *defaults
  nodeGroup: "master"
  enabled: true
  # esConfig:
  #   elasticsearch.yml: |
  #     xpack:
  #       security:
  #         enabled: false
  #         transport:
  #           ssl:
  #             enabled: false
  #         http:
  #           ssl:
  #             enabled: false
  roles:
    master: "true"
    ingest: "false"
    data: "false"
  esJavaOpts: "-Xms128m -Xmx128m"
  resources:
    requests:
      memory: 128Mi
      cpu: "10m"
    limits:
      memory: "1Gi"
      cpu: "2"
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

kibana:
  enabled: false
  elasticsearchHosts: http://my-elasticsearch-hot-url:9200  # Change me
  resources:
    requests:
    limits:
      cpu: 2
      memory: 2Gi
  kibanaConfig:
    kibana.yml: |
      server.name: kibana
      server.host: "0"
      elasticsearch.requestTimeout: 180000
  ingress:
    enabled: false
    # hosts:
    #   - host: kibana.setme.org
    #     paths:
    #       - path: /
    # tls:
    #   - hosts:
    #       - kibana.setme.org
    #     secretName: kibana.setme.org-tls
  readinessProbe:
    successThreshold: 1

prometheus-elasticsearch-exporter:
  enabled: true
  es:
    uri: http://my-elasticsearch-master-url:9200  # Change me
  serviceMonitor:
    enabled: false
  prometheusRule:
    enabled: false
    rules:
      - alert: ESClusterStatusRED
        expr: |
          elasticsearch_cluster_health_status{color="red", service="{{ template "elasticsearch-exporter.fullname" . }}"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: 'Cluster health status is RED'
          description: 'Cluster {{ "{{ $labels.cluster }}" }} health status is RED'
      - alert: ESClusteStatusYELLOW
        expr: |
          elasticsearch_cluster_health_status{color="yellow", service="{{ template "elasticsearch-exporter.fullname" . }}"} == 1
        for: 20m
        labels:
          severity: high
        annotations:
          summary: 'Cluster health status is YELLOW'
          description: 'Cluster {{ "{{ $labels.cluster }}" }} health status is YELLOW'

      # Report any rejected request.
      - alert: ESBulkRequestsRejection
        expr: |
          irate(elasticsearch_thread_pool_rejected_count{service="{{ template "elasticsearch-exporter.fullname" . }}"}[1m]) > 0
        for: 1s
        labels:
          severity: high
        annotations:
          summary: 'Elasticsearch Bulk Query Rejection'
          description: |
            'Bulk Rejection at {{ "{{ $labels.name }}" }} node in {{ "{{ $labels.cluster }}" }} cluster'

      - alert: ESJVMHeapHigh
        expr: |
          sum by (cluster, name, namespace)
            (elasticsearch_jvm_memory_used_bytes{area="heap", service="{{ template "elasticsearch-exporter.fullname" . }}"}
            /
            elasticsearch_jvm_memory_max_bytes{area="heap", service="{{ template "elasticsearch-exporter.fullname" . }}"}
          )
          > 75
        for: 5m
        labels:
          severity: alert
        annotations:
          summary: 'JVM Heap usage on the node is high'
          description: |
            'JVM Heap usage on the node {{ "{{ $labels.node }}" }} in {{ "{{ $labels.cluster }}" }} cluster is {{ "{{ $value }}" }}%. There might be long running GCs now.'

cerebro:
  enabled: false
  ingress:
    enabled: false
  config:
    basePath: '/'
    restHistorySize: 50
    hosts:
      - name: logging
        host: http://my-elasticsearch-hot-url:9200  # Change me
