{{- if and .Values.postgresql.enabled .Values.postgresql.backup.s3.enabled }}

kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ include "druid.postgresql-s3-backup.fullname" . }}
  labels:
    {{- include "druid.common-labels" . | nindent 4 }}
    app.kubernetes.io/component: {{ .Values.postgresql.backup.s3.cronJob.name }}
data:
  backup.sh: |
    #!/bin/bash
    # Project at https://github.com/gabfl/pg_dump-to-s3
    set -e
    set -x

    mkdir -p /var/lib/apt/lists/partial && apt update && apt install -y awscli

    # Set current directory
    DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

    # Vars
    NOW=$(date +"%Y-%m-%d-at-%H-%M-%S")
    DELETETION_TIMESTAMP=$(date +%s --date="-$DELETE_AFTER") # Maximum date (will delete all files older than this date)

    # Split databases
    IFS=',' read -ra DBS <<< "$PG_DATABASES"

    # Delere old files
    echo " * Backup in progress.,.";

    # Loop thru databases
    for db in "${DBS[@]}"; do
        FILENAME="$NOW"_"$db"

        echo "   -> backing up $db..."

        # Dump database
        pg_dump -Fc -h $PG_HOST -U $PG_USER -p $PG_PORT $db --format=custom -w > /tmp/"$FILENAME".psql

        # Copy to S3
        aws s3 cp /tmp/"$FILENAME".psql s3://$S3_PATH/"$FILENAME".dump --storage-class STANDARD_IA

        # Log
        echo "      ...database $db has been backed up"
    done

    # Delere old files
    echo " * Deleting old backups...";

    # Loop thru files
    aws s3 ls s3://$S3_PATH/ | while read -r line;  do
        # Get file creation date
        createDate=`echo $line|awk {'print $1" "$2'}`
        createDate=`date -d"$createDate" +%s`

        if [[ $createDate -lt $DELETETION_TIMESTAMP ]]
        then
            # Get file name
            FILENAME=`echo $line|awk {'print $4'}`
            if [[ $FILENAME != "" ]]
            then
                echo "   -> Deleting $FILENAME"
                aws s3 rm s3://$S3_PATH/$FILENAME
            fi
        fi
    done;

    echo ""
    echo "...done!";
    echo ""

{{- end }}
